Statistics as a mathematical field has a long history as a tool for characterizing social, economic and scientific phenomena. One of the most used statistical methods is regression analysis \citep{GLMM_book_old, Ryan_statistical_methods}, which is used to model the relationship between a response variable and one or more covariates. To understand this relationship, researchers often want to determine whether a covariate is associated with the response, and to what extent. The exploration of this fundamental question has lead to a number of statistical methods trying to answer it. An agreement on a single method has not been reached, with the topic still being debated and actively researched. 
\\
\\
A pioneer in statistics, Ronald Fisher, introduced the concept of the $p$-value almost one hundred years ago \citep{Fisher1925}. To this day, the $p$-value is arguable the most widespread and used method, to determine if a covariate is \textit{statistically significant} with respect to the response. Popularly, to determine statistical significance, a hypothesis test is performed and the resulting $p$-value is compared to a threshold level $\alpha$. Typically, if the $p$-value is smaller than $\alpha$, the covariate is considered statistically significant. However, this way of determining statistical significance, which often mistakenly is interpreted as importance, is very prone to misinterpretations, and subject to great criticism \citep{benjamin2018redefine}. 
\\
\\
Recently, the social and biomedical sciences have been subject to a reproducibility crisis, in which published results cannot be reproduced \citep{gelman_crisis}. One possible solution is suggested by $72$ authors in \citet{benjamin2018redefine}, which is to lower the typical significance level from $\alpha=0.05$ to $\alpha=0.005$. This could be a solution, however \citet{gelman_crisis} sees this as a quick fix, which will not solve the underlying problem. Instead, it is proposed to simply abandon the term statistical significance, and not force results to be based on a threshold value which gives rigid and binary answers. The remedy, according to \citet{gelman_crisis}, is to rather interpret the $p$-value as a continuous measure of evidence, among many others. Going forward, the thesis will not consider statistical significance but rather statistical evidence to avoid the hazards by using such rigid interpretations.
\\
\\
There exists many other measures to compliment the $p$-value when assessing a statistical model. As in \citet{Arnstad:Relative_variable_importance_in_Bayesian_linear_mixed_models:2024}, we list some of the most common measures
\begin{itemize}
    \item \textbf{Effect sizes:} By looking at the squared value of the standardized regression coefficients, one can determine the effect size of the covariates. For independent covariates, the effect size coincides with the proportion of variance explained by the covariate. The effect sizes are a good measure for uncorrelated covariates, but falls short when correlation makes the coefficient estimates unstable.
    \item \textbf{Confidence intervals:} Confidence can be calculated for the effect sizes, and the interval can be used to determine a range of values that can be seen as statistically consistent with the data. This can be useful, but confidence intervals are frequently used as a tool to perform hypothesis tests, and the conclusion effectively relies on the $p$-value.
    % The downside to using confidence intervals is that one effectively relies on the $p$-value to determine it, and therefore it does not provide any additional information than using the $p$-value.
    \item \textbf{Information criteria:} The AIC \citep{Akaike_AIC} and the BIC \citep{Schwarz_BIC} are information criteria that use the likelihood function to compute a goodness of fit statistic. These can be used to assess the information contained in the model, and can be used to assess the unique information that one covariate contributes to the model. A general problem with information criteria when used for model selection, is that they tend to introduce bias in the selection. Further, the thresholds used for goodness of fit are often closely related to the $p$-values \citep{Murtaugh2014}.  %Again, the criteria fall short when correlation is present, as it cannot take shared information between covariates into account. 
    \item \textbf{Bayes factor:} As an alternative to the $p$-value, Bayes factor can be used to assess the evidence from the data to either support a hypothesis or not. Bayes factor is therefore less rigid than the $p$-value, and is rather a continuous measure of evidence.
    \item \textbf{Decomposing the $R^2$:} The $R^2$ measures the variance explained in the response by the covariates. As such, it is a goodness of fit measure and can be decomposed into a share from each covariate in the model to determine the variance explained by each covariate. The $R^2$ is widely used and intuitive, but a proper decomposition of the value with correlated covariates is not straightforward.
\end{itemize}
The methods listed all have in common that they become less interpretable for correlated covariates. As correlation is a common feature of many datasets, especially from the real world, this is a general problem that regression models are not well suited to handle \citep{Gromping_2015}. 
\\
\\
By decomposing the $R^2$ value and assigning each variable with a share of explained variance in the response, we have a measure of the relative importance of each covariate \citep{gromping_relaimpo}. The problem of decomposing the $R^2$ in a sensible manner has lead to various approaches. One of the most rigorous methods is the approach by Lindemann, Merenda and Gold (LMG) \citep{Lindeman1980}, which decomposes the $R^2$ value by considering all possible orderings of the covariates. As covariates are added to the null model, the average increase in $R^2$ for each permutation is calculated and each covariate is assigned a share of relative variable importance. As the LMG method is very popular, it has been applied in dominance analysis \citep{budescu1993dominance} and coincides with the Shapley value in game theory \citep{Shapley1953StochasticG,Lipovetsky_GameTheory}. The LMG method has proven to be consistent for the linear regression, but is computationally expensive and therefore in some cases not feasible \citep{gromping_relaimpo}. 
\\
\\
Another, approximatemethod denoted as \textit{relative weights method} \citep{johnson_minimization_trace, Fabbris1980, Genizi_relative_weights} can be seen as an approximation of the LMG method, to remedy the computational burden. By projecting the covariates into an orthogonal space and then conducting the analysis on the projected covariates, before transforming them back to the original covariate space, the relative weights method efficiently decomposes the $R^2$ value. At the cost of approximating rather than being more rigorous, the relative weights method is able to handle larger models and is therefore preferred if the LMG is not feasible \citep{gromping_relaimpo}.
\\
\\ 
Both the LMG method and the relative weights method are originally designed to decompose the $R^2$ for linear regression. However, for many scenarios, a linear regression is not sufficient to model the relationship between the response and the covariates. Both models were extended in the likelihood-based framework by \citet{matre}, so that they could be applicable for models including random intercepts (that is, linear mixed models), but we are not aware of any further extensions. Another problem is that there is little consensus on how to properly define the $R^2$ for more complex methods. A simple and intuitive definition of the $R^2$ for more general regression models was suggested by \citet{nakagawa2013general} and serves as the basis for the extensions of the LMG and relative weights method in \citet{matre}. 
\\
\\
Although general relative variable importance methods are hard to come by for complex models, there are some methods that have been developed for specific purposes. For instance, for hierarchical models, the R package \texttt{rptR} is developed by \citet{Stoffel2017rptR} and designed to calculate the intra-class correlation (ICC) of observations belonging to the same hierarchical level. This package is introduced by considering the repeatability of phenotypic traits, which is the ICC in field of ecology and evolution \citep{Stoffel2017rptR}. The repeatability of a trait can be seen as a particular instance of relative variable importance, as it is the result of a variance decomposition. Using the \texttt{rptR} package allows for repeatability calculations for binomial and Poisson generalized linear mixed models, as well as for linear mixed models. The package uses the $R^2$ definition from \citet{nakagawa2013general} and is a valuable step towards a general relative variable importance measure.
\\
\\
The above discussed methods are all derived and implemented in the frequentist (likelihood) framework. However, the Bayesian framework has significant advantages when compared to the frequentist framework \citep{robert2007bayesian} and has had a surge in popularity due to the recent years advancements in computational capabilities \citep{hackenberger2019bayes}.
% The treatment of parameters as random variables in the Bayesian framework, rather than point estimates, naturally includes moments such as variance in the posterior distribution.
Unlike the frequentist framework, the Batesian framework treats parameters as random variables rather than point estimates, naturally includes moments such as variance in the posterior distribution. As the Bayesian framework has become more available, researchers can obtain more inference and thereby make more informed decisions. Therefore, we believe that the Bayesian framework provides a preferable framework for assessing the statistical evidence of covariates. Moreover, there are numerous useful applications for relative variable importance, as decomposing the $R^2$ is desirable in various fields. For instance, in quantitative genetics, researchers often aim to determine the heritability of phenotypic traits. Heritability is a key measure used to explain how the mean value of a trait changes, and can thereby help us better understand evolution. Although point estimates of heritability are fairly straightforward to obtain, it is considerably more difficult to evaluate the uncertainty of the estimates \citep{Stoffel2017rptR}. We believe that the Bayesian framework is the natural choice for quantifying this uncertainty, demonstrating its practical benefits. This particular example has been a great motivation behind our attempt to develop a Bayesian relative variable importance measure.
\\
\\
%In the Bayesian framework, the field of relative variable importance is small. 
In the Bayesian framework, there is not much literature or research on relative variable importance. The LMG and relative weights method are both based on the frequentist framework, and the Bayesian framework has not been explored to the same extent. One possible line of action are the Generalized decomposition priors on $R^2$ (GDR2) \citep{aguilar2024generalized}, which are based on the $R^2$-induced Dirichlet decomposition (R2D2) priors \citep{zhang2020bayesian}. By placing a prior on the $R^2$ value, the R2D2 priors proceed with a Dirichlet decomposition of the $R^2$ value to be able to assign each covariate with a share of relative variable importance. The GDR2 priors are a generalization of the R2D2 priors which performs the decomposition using logistic normal distributions \citep{aguilar2024generalized}. At the time being, the R2D2 and GDR2 priors have been applied only to linear regression, with a focus on obtaining trustworthy predictions. Therefore, these methods have not been used explicitly as relative importance measures, and it is not clear how they generalize to more complex models. Nonetheless, they serve as an important contribution to the Bayesian framework for relative variable importance.
\\
\\
To summarize, the field of relative variable importance offers a wide range of methods to determine the statistical evidence of covariates in the simple linear regression model. However, correlation between the covariates is troublesome and can lead to unreliable results. Further, for more complex models, there is little consensus on how computations should be carried out and therefore a lack of robust methods. Many of the existing methods have been developed for specific purposes and do not pose a general method for relative variable importance. In the Bayesian framework, there has been done little work on relative variable importance, despite its advantageous features for statistical inference.
\\
\\
This thesis aims to develop a relative variable importance measure in the Bayesian framework, which can be easily implemented by researchers in different fields. The measure should be applicable to a wide range of regression models with complex structures, with an emphasis on interpretable and reliable results. The method uses the relative weights method as a basis, and applies the method in the Bayesian framework. To calculate the relative variable importance measures, the method merges the $R^2$ definition for GLMMs of \citep{nakagawa2013general} and \citep{nakagawa2017}, with the Bayesian $R^2$ for the linear regression model defined by \citet{gelman2017rsquared}. The results obtained are in the form of approximated posterior distributions, allowing for uncertainty in the estimates to be quantified. Our hope is that these distributional results are easy to interpret and allow researchers to quantify the uncertainty, rather than using a threshold-based method. For the method to be easily used, it has been implemented as a package in R. The package is called \texttt{BayesianVariableImportance} and is available, along with installation and usage examples, on the authors GitHub \url{https://github.com/AugustArnstad/BayesianVariableImportance}. 
\\
\\
% In this thesis, the field of relative variable importance will be explored, and we propose a Bayesian method for calculating the relative variable importance. The author proposed a Bayesian analogue to the extensions of the LMG and relative weights method in \citet{Arnstad:Relative_variable_importance_in_Bayesian_linear_mixed_models:2024}, which focused on linear mixed models. To address some of its limitations, we have now considered the larger class of generalized linear mixed models and for this class defined what we believe to be a sensible definition of the $R^2$. The method has applied the relative weights method in the Bayesian framework to fit a GLMM, returning results in the form of distributions. Our hope is that these distributional results will be easy to interpret and allow researchers to interpret the uncertainty, rather than using a threshold based method. For the method to be easily used, it has been implemented as a package in R. The package is called \texttt{BayesianVariableImportance} and is available, along with installation and usage examples, on the authors Github \url{https://github.com/AugustArnstad/BayesianVariableImportance}. 
% \\
% \\
As regression models are perhaps the most used statistical modelling tool, the span of applications for the BVI method is virtually unlimited. Given that the regression model is a suitable choice, the BVI method can be useful in fields such as biology, economics, social sciences, medicine and more. Research in many of these fields is crucial if we are to reach the $17$ goals for sustainability set by the United Nations \citep{un_sdg_goals}. Here, we particularly highlight and illustrate the application of the BVI method for use in quantitative genetics. The overarching question for such analysis is to better understand evolution and how species develop when subject to different and changing environments. Answers to such questions will be crucial to reach goal $14$ and $15$ of the United Nations sustainability goals, which is about life in the ocean and on land. The aim of goal $14$ is to \textit{Conserve and sustainably use the oceans, seas and marine resources for sustainable development} and goal $15$ is about \textit{Protecting, restoring and promoting sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss} \citep{un_sdg_goals}. Further, we believe that the BVI method can give insight that can be useful for numerous of these goals, such as goal $3$ about \textit{Good health and well-being}, goal $7$ about \textit{Affordable and clean energy} and goal $13$ about \textit{Climate action}.
\\
\\
The structure of the thesis is as follows. In \Cref{ch:theory} we look at some background theory and put forth the theoretical results that will be used in the method. To describe our calculations, \Cref{ch:method} presents the methodology and logic of our contribution. Evaluating the method is done in \Cref{ch:results}, where we look at a simulation study, a case study and apply the method to a real dataset. We discuss the findings in \Cref{ch:discussion} and conclude the thesis in \Cref{ch:conclusion}. In \Cref{ap:github-repository} we give the link to the authors GitHub repository for the R package and the thesis, \Cref{ap:bayesian-importance} contains a usage example of the R package and \Cref{ap:supplementary} has some supplementary material. A miscellaneous proof is found in \Cref{ap:proofs}. 
\\
\\
Please note that this thesis continues the work done by the author in \citet{Arnstad:Relative_variable_importance_in_Bayesian_linear_mixed_models:2024}, and therefore some sections overlap. Following the guidelines of the Institute of Mathematical Sciences, stating that sections need not be rewritten, some sections are the same, or slightly modified, as in the project thesis.
% Add the specific sections?