Statistics as a mathematical field has a long history as a tool for characterizing social, economic and scientific phenomena. One of the most used statistical methods is regression analysis \citep{Gromping_2015}, which is used to model the relationship between a response variable and one or more covariates. To understand this relationship, researchers often want to determine whether a covariate is associated with the response, and to what extent. The exploration of this fundamental question has lead to a number of statistical methods trying to answer it. An agreement on a single method has not been reached, with the topic still being debated and actively researched. 
\\
\\
A pioneer in statistics, Ronald Fisher, introduced the concept of the $p$-value almost one hundred years ago \citep{Fisher1925}. To this day, the $p$-value is arguable the most widespread and used method, to determine if a covariate is \textit{statistically significant} with respect to the response. Popularly, to determine statistical significance, a hypothesis test is performed and the resulting $p$-value is compared to a threshold level $\alpha$. Typically, if the $p$-value is smaller than $\alpha$, the covariate is considered statistically significant. However, this way of determining statistical significance is very prone to misinterpretations, and is subject to great criticism \citep{benjamin2018redefine}. 
\\
\\
Recently, the social and biomedical sciences have been subject to a reproducibility crisis, in which published results cannot be reproduced \citep{gelman_crisis}. One possible solution is suggested by $72$ authors in \citet{benjamin2018redefine}, which is to lower the typical significance level from $\alpha=0.05$ to $\alpha=0.005$. This could be a solution, however \citet{gelman_crisis} sees this as a quick fix, which will not solve the underlying problem. Instead, it is proposed to simply abandon the term statistical significance, and not force results to be based on a threshold value which gives rigid and binary answers. The remedy, according to \citet{gelman_crisis}, is to rather interpret the $p$-value as a continuous measure of evidence, among many others. Going forward, the thesis will not consider statistical significance but rather statistical evidence to avoid the hazards by using such rigid interpretations.
\\
\\
There exists many other measures to compliment the $p$-value when assessing a statistical model. As in \citet{Arnstad}, we list some of the most common measures
\begin{itemize}
    \item \textbf{Effect sizes:} By looking at the squared value of the standardized regression coefficients, one can determine the effect size of the covariates. For independent covariates, the effect size coincides with the proportion of variance explained by the covariate. The effect sizes are a good measure for uncorrelated covariates, but falls short when correlation makes the coefficient estimates unstable.
    \item \textbf{Confidence intervals:} Confidence can be calculated for the effect sizes, and the interval can be used to determine a range of values that can be seen as statistically consistent with the data. The downside to using confidence intervals is that one effectively relies on the $p$-value to determine it, and therefore it does not provide any additional information than using the $p$-value.
    \item \textbf{Information critera:} The AIC \citep{Akaike_AIC} and the BIC \citep{Schwarz_BIC} are information criteria that use the likelihood function to compute a goodness of fit statistic. These can be used to assess the information contained in the model, and can be used to assess the unique information that one covariate contributes to the model. Also here, the critera fall short when correlation is present, as it cannot take shared information between covariates into account. 
    \item \textbf{Bayes factor:} As an alternative to the $p$-value, Bayes factor can be used to assess the evidence from the data to either support a hypothesis or not. Bayes factor is therefore less rigid than the $p$-value, and is rather a continuous measure of evidence.
    \item \textbf{Decomposing the $R^2$:} The $R^2$ measures the variance explained in the response by the covariates. As such, it is a goodness of fit measure and can be decomposed into a share from each covariate in the model to determine the variance explained by each covariate. The $R^2$ is widely used and intuitive, but a proper decomposition of the value with correlated covariates is not straightforward.
\end{itemize}
The methods listed all have in common that they become less reliable for correlated covariates. As correlation is a common feature of many datasets, especially from the real world, this is a general problem that regression models are not well suited to handle \citep{Gromping_2015}. 
\\
\\
By decomposing the $R^2$ value and assigning each variable with a share of explained variance in the response, we have a measure of the relative importance of each covariate \citep{gromping_relaimpo}. The problem of decomposing the $R^2$ in a sensible manner has lead to many different approaches. One of the most rigorous methods is the approach by Lindemann, Merenda and Gold (LMG) \citep{Lindeman1980}, which decomposes the $R^2$ value by considering all possible orderings of the covariates. As covariates are added to the null model, the average increase in $R^2$ for each permutation is calculated and each covariate is assigned a share of relative variable importance. As the LMG method is very popular, it has been applied in dominance analysis \citep{budescu1993dominance} and coincides with the Shapley value in game theory (\citep{Shapley1953StochasticG}, \citet{Lipovetsky_GameTheory}). The LMG method has proven to be consistent for the linear regression, but is computationally expensive and therefore in some cases not feasible. 
\\
\\
The relative weights method (\citet{johnson_minimization_trace}, \citet{Fabbris1980}, \citet{Genizi_relative_weights}) can be seen as an approximation of the LMG method, to remedy the computational burden. By projecting the covariates into an orthogonal space and then conducting the analysis on the projected covariates, before transforming them back to the original covariate space, the relative weights method efficiently decomposes the $R^2$ value. At the cost of approximating rather than being more rigorous, the relative weights method is able to handle larger models and is therefore preferred if the LMG is not feasible \citep{gromping_relaimpo}.
\\
\\ 
Both the LMG method and the relative weights method are originally designed to decompose the $R^2$ for the linear regression. However, for many scenarios, a linear regression is not sufficient to model the relationship between the response and the covariates. Both models were extended in \citet{matre}, so that they could be applicable for the random intercept models. Another problem is that there is little consensus on how to properly define the $R^2$ for more complex methods. A simple and intuitive definition of the $R^2$ for more general regression models was suggested by \citet{nakagawa2013general} and serves as the basis for the extensions of the LMG and relative weights method. 
\\
\\
In the Bayesian framework, the field of relative variable importance is small. The LMG and relative weights method are both based on the frequentist framework, and the Bayesian framework has not been explored to the same extent. One possible line of action are the Generalized decomposition priors on $R^2$ (GDR2) \citep{aguilar2024generalized}, which are based on the $R^2$-induced Dirichlet decomposition (R2D2) priors \citep{zhang2020bayesian}. By placing a prior on the $R^2$ value, the R2D2 priors proceed with a Dirichlet decomposition of the $R^2$ value to be able to assign each covariate with a share of relative variable importance. The GDR2 priors are a generalization of the R2D2 priors which performs the decomposition using logsitic normal distributions \citep{aguilar2024generalized}. At the time being, the GDR2 priors have been applied to the linear regression, with a focus on obtaining trustworthy predictions. Therefore, it is not clear how the GDR2 priors can be used for relative variable importance and if they can be extended to calculate relative variable importance for more complex regression models. Nonetheless, they serve as an important contribution to the Bayesian framework for relative variable importance.
\\
\\
The Bayesian framework has significant advantages when compared to the frequentist framework \citep{robert2007bayesian} and has had a surge in popularity due to the recent years advancements in computational capabilities \citep{hackenberger2019bayes}. The treatment of parameters as random variables in the Bayesian framework, rather than point estimates, naturally includes moments such as variance in the estimation procedure. As the Bayesian framework has become more available, researchers can obtain more inference and thereby make more informed decisions. Therefore, we believe that the Bayesian framework provides a better platform for assessing the statistical evidence of covariates. This is the motivation behind our attempt to develop a relative variable importance measure in the Bayesian framework.
\\
\\
In this thesis, the field of relative variable importance will be explored, and we propose a Bayesian method for calculating the relative variable importance. The author proposed a Bayesian analogue to the extensions of the LMG and relative weights method in \citet{Arnstad}, which focused on linear mixed models. To address some of its limitations, we have now considered the larger class of generalized linear mixed models and for this class defined what we believe to be a sensible definition of the $R^2$. The method has applied the relative weights method in the Bayesian framework to fit a GLMM, returning results in the form of distributions. Our hope is that these distributional results will be easy to interpret and allow researchers to interpret the uncertainty, rather than using a threshold based method. For the method to be easily used, it has been implemented as a package in R. The package is called \texttt{BayesianVariableImportance} and is available, along with installation and usage examples, on the authors Github \url{https://github.com/AugustArnstad/BayesianVariableImportance}. 
\\
\\
The structure of the thesis is as follows. In \Cref{ch:theory} we look at some background theory and put forth the theoretical results that will be used in the method. To describe our calculations, \Cref{ch:method} presents the methodology and logic of our contribution. Evaluating the method is done in \Cref{ch:results}, where we look at a simulation study, a case study and apply the method to a real dataset. We discuss the findings in \Cref{ch:discussion} and conclude the thesis in \Cref{ch:conclusion}. In \Cref{ap:github-repository} we give the link to the authors Github repository for the R package and the thesis, \Cref{ap:bayesian-importance} contains a usage example of the R package and some miscalculations in the \Cref{ap:proofs}. 