

The work presented in this thesis partially builds on the authors previous work in \citet{Arnstad} and has developed the Bayesian Variable Importance method to put forward a novel framework for estimating relative variable importance in generalized linear mixed models. 
As a result, the BVI method is now capable of handling complex models, while at the same time proving to be computationally efficient. A Bayesian approach to variable importance has involved utilizing the relative weights method \citep{johnson_relative_weights} to project the fixed covariates into an orthogonal space. The projection, or approximation, of these covariates are used to fit the model, before a back-transformation is applied to relate the estimated results back to the original covariate space. To obtain inference on the Bayesian GLMMs, we have translated frequentist concepts, such as the $R^2$ measure, to fit in the Bayesian framework. This translation has been inspired by the work in \citet{gelman2017rsquared}, but is also a result of the authors own work. Once the methodology was developed, a simulation study was conducted in which the underlying structure was known. After a simulation study, the method was applied to a case study in which we could compare the model to a similar relative variable importance measure, \texttt{rptR}, in the frequentist framework. Lastly the method was applied to a dataset gathered from house sparrows on Helgelandskysten, Norway, to investigate the heritability properties of the sparrow population. The methodology has been implemented in an R package, \texttt{BayesianVariableImportance}, which is available in full on the authors Github, with a link to the repository provided in \Cref{ap:github-repository}. In \Cref{ap:bayesian-importance} a usage example of the package is supplied, which is also available on the authors Github along with all code used to obtain the results of this thesis.
\\
\\
Being a general method, our aspirations are that the BVI method will be applied by researches across disciplines that are interested in the statistical properties of covariates in GLMMs. The BVI method does not aim to give researchers an exact measure of variable importance, but rather provide posterior distributions of relative importance that should be interpreted by the researcher in the field of application. As the distributions will naturally have an uncertainty, it is advantageous if this uncertainty is assessed and understood as a part of the analysis. Hopefully, this can give broader inference on the importance of the covariates, which will in turn lead to more informed conclusions on the effect of covariates on a response. In itself, the BVI poses an analogue to the frequentist relative variable importance measure \texttt{rptR} for non-Gaussian responses, but with the added benefit of directly estimating the relative importance distributions of fixed effects. Further, for gaussian data, it also poses an analogue to more established methods such as the LMG method \citep{gromping_relaimpo}, the extended LMG method \citep{matre} and the extended relative weights method \citep{matre} as discussed in \citet{Arnstad}. Lastly, the BVI method allows one to specify covariance structures in the random effects, which can be beneficial when modeling complex data structures.
\\
\\
The field of Bayesian variable importance measures for regression models is not very large, but there has been some research on the topic. Specifically, the use of continuous shrinkage priors for linear models of high dimension has attracted attention \citep{aguilar2024generalized}. One prior that can be applied as a continuous shrinkage prior and that has favorable properties for variable importance is the $R^2$-induced Dirichlet Decomposition (R2D2) priors \citep{zhang2020bayesian}. The R2D2 prior places a prior on the $R^2$ of the model, ($R^2$ has a similar definition as that of \citet{gelman2017rsquared}), and then performs a Dirichlet decomposition to allocate the coefficients with a share of explained variance \citep{aguilar2024generalized}. Using Dirichlet priors on coefficients is seen as a way of assigning them with a very interpretable distribution, because of the way a Dirichlet distribution is parametrized. Specifically, the expected value of the coefficient when assigned a Dirichlet distribution, can be seen as the relative, a priori, importance of the coefficient \citep{aguilar2024generalized}. For hyperparameters $(\alpha_1, ..., \alpha_K)$ and coefficients $(\phi_1, ..., \phi_K)$ equipped with a Dirichlet prior, we have $\mathbb{E}[\phi_k]=\frac{\alpha_k}{\sum_{k=1}^K\alpha_k}$. The Dirichlet distribution has a tendency to move towards negative correlation structures that are completely dependent on the expected value of each coefficient. Therefore, R2D2 priors were further developed into the Generalized Decomposition $R^2$ (GDR2) priors \citep{aguilar2024generalized}. The GDR2 priors allow for a more flexible prior than the Dirichlet to be placed for decomposing the variance explained. It also overcomes the obstacle of only considering negative correlation structures by applying the logisitis normal prior \citep{aguilar2024generalized}. A main point of concern for the GDR2 priors are that they can pose a lot of constraints on the covariance structure, which may not be suitable in practice \citep[and references therein]{aguilar2024generalized}. The GDR2 approach has mostly been applied to obtain trustable prediction models for high dimensional linear regression models \citep{aguilar2024generalized}, but we believe that this could also be developed further to possibly serve as a variable importance measure. The GDR2 differs from the BVI method in some ways. Firstly, to our knowledge, it has yet to be applied for GLMMs and so direct comparison for the most complex models is difficult. Further, the GDR2 priors are continuous shrinkage priors, which are designed to shrink small effects towards zero. We use penalizing complexity priors, which puts the emphasis on the complexity of the model, and so the general idea is similar but the implemented priors are different. Moreover, we sample values of coefficients and random effects a posteriori and then estimate the relative importances based on the samples. As the R2D2 and GDR2 priors have mainly been applied to prediction scenarios, it is not immediately clear how the specific variable importances would be calculated. 

MORE?
\\
\\
For relative variable importance measures, some criteria are found in \Cref{sec:rel_imp} that it is desirable to fulfill. It was argued that the simulation study in \citet{Arnstad} gave promising results of the BVI method fulfilling the proper decomposition criteria. When assessing how the BVI method performs on GLMMs, in which the response variance is not on the same scale as the covariates, this criteria is hard to assess. Instead of aiming to decompose the total model variance, we find it natural to rather aim for a proper decomposition of the models $R^2$ on the latent scale. From the definition of $R^2$ for GLMMs in \citet{nakagawa2013general}, the simulation study shows that the posterior distributions of the marginal and conditional $R^2$ are generally symmetrically distributed around the expected $R^2$ value. When the correlation between fixed effects is $0.4$, we see that the $R^2$ estimates from the Poisson model are slightly smaller on average than the expected value. The average is of course effected by the model fitting problems causing the estimated $R^2$ values to be artificially small. Therefore, it is plausible that for a good model fit, the BVI method will give estimate the $R^2$ values closer to what one might expect. Based on these observations, we argue that the BVI method, in posterior expectation, is capable of providing a satisfactory decomposition of the $R^2$ in GLMMs. The non-negativity criteria is fulfilled by recalling that the relative importance estimates of fixed effects are squared, and no variance estimate for random effects can be negative. Consequently, the posterior relative importance distributions will not contain negative values. As discussed in \citet{Arnstad}, the exclusion criteria will not be used in our assessment, as \citet{gromping_relaimpo} argues this is not in general reasonable. Lastly, violating  the inclusion criteria is seen as unlikely to occur in practice, although it is mentioned in \citet{matre} that the extensions of the LMG method and the relative weights method can violate this criteria. It has not yet been properly assessed how the inclusion criteria applies to the BVI method. A suggestion that was debated in \citep{Arnstad} is whether one should directly translate the desirable criteria for relative importance measures in the frequentist framework to the Bayesian framework. In the case of the inclusion criteria, we interpret this to mean that if the posterior relative importances of a non-zero regressor contains zero, this is a violation the criteria. The Bayesian framework is designed to provide uncertainty, and therefore subjecting its result against a rigid threshold of containing or not containing zero is not necessarily reasonable. By not considering the inclusion criteria, the inclusion of zero values in the relative importance distributions of a non-zero regressor would require the researcher to carefully assess the covariate. A careful evaluation of the results is in line with what we intend the BVI method to invoke, and therefore the violation of the inclusion criteria might not pose a problem at all. With this in mind, the results of the simulation study show that the BVI method produces results that align well with what we expect, and that the results are plausible. Therefore, we believe that for most practical applications, the general idea behind the criteria of variable importance measures are fulfilled by the BVI method. 
\\
\\
It could be questioned if our investigations of the Bayesian Variable Importance method has been sufficient. For example, in the simulation study we do not allow for more extreme correlation levels than $-0.4$ and $0.4$. This is not a very large value, and therefore some analysis on the method for higher correlation levels could be of interest. The reasoning behind using such moderate correlation levels, is that the model fitting procedure was often compromised for more correlated covariates. As mentioned in the results, we experienced model crashes for a correlation of $0.4$, and the frequency of crashes rose when testing with higher correlation. It is therefore not clear how well the model will perform if covariates share much information, but results for an LMM with correlation levels of $0.9$ can be found in \citet{Arnstad}. Further, our initial idea was to also include a binomial model with the probit link function in the simulation thesis. This idea was abandoned due to severe model fitting problems, in which INLA would not converge. This was of course unfortunate, but as the result could not be trusted, we chose to ommit them from the thesis. We do however believe that if the probit model had been a good fit, the method would be able to calculate the relative importances in a similar manner as for the logit model.
\\
\\
Another topic of discussion, regards choice of priors. It would be natural, given more time, to investigate how different priors would perform and also if one could tune the hyperparameters of the priors applied. As priors are a large subject in themselves, a thorough analysis of prior effects on the BVI method was not performed. We chose to follow the recommendations of \citet{simpson2017penalising} to use penalizing complexity priors, as these had desirable properties and are designed to nicely fit INLA models \citep{simpson2017penalising}. The parameters of our PC priors follow the default values in the \texttt{R-INLA} package, and we have not investigated how these could be tuned to better fit the data. This could be done to further solidify the results of the BVI method, but would also require more time and resources than what was available in the scope of this thesis. 
\\
\\
Many of the foundational calculations made in the BVI method relies on approximations. The relative weights method can be viewed an approximation of the Lindemann, Merenda and Gold (LMG) method, and the accuracy of INLA is dependent on how well the marginals are approximated. It is to be expected that the errors made in these approximations are propagated to the outputted results of the BVI method. However, we argue that the results obtained from the BVI method are satisfactory. For the simulation study, the results align well with our expectation in cases where we can give an expectation. Further, the patterns for varying correlation levels seem to be logical, and the results plausible, even though a true value is hard to obtain. A worrying aspect of the simulation study was the results for the highest correlation level in the Poisson model. The models that estimated all fixed coefficients to be zero are not sensible, and their origin is not clear. Investigations into the cause of these problems showed that such models with zero fixed coefficients spontaneously appear when fitting a large number of INLA models. We believe that the trouble here lies with the INLA fitting procedure, and we therefore stress the need to make sure the model is a good fit before interpreting the posterior relative variable importance distributions. The BVI method seems to provide very similar results to the \texttt{rptR} package which is a comparable method. In addition to give similar results, the BVI method proves to be much faster than the frequentist counterpart in \texttt{rptR}. The computational gain is mostly due to the use of sampling rather than bootstrapping, which we hope will be a benefit for researchers who wish to apply the method. Perhaps the most positive result, is the performance on real data from the house sparrow study. The method gives strikingly similar estimates as those done by domain experts, further solidifying our belief that the methodology and implementation is sound. However, the posterior distributions of relative variable importance for the house sparrow study are not very smooth and returns a pattern that is hard to interpret. Nonetheless, based on the results we have believe that applying the BVI method is accurate enough to be viewed as drawing samples from a distribution centered around the true value.
ADD SOMETHING HERE? NEED TO ASK STEFFI ABOUT WHAT SHE THINKS THIS IS THE RESULT OF.
\\
\\
We are not at the time aware of a similar variable importance tool for Bayesian GLMMs as the BVI method. Therefore, there is still much work to be done in this field, and many opportunities for expanding the BVI method. Currently we have implemented the BVI method to handle Gaussian, Binomial and Poisson distributed responses, but there are a number of other distributions that could be of interest. In \citet{nakagawa2017}, the quasi-Poisson, negative Binomial and Gamma distributions are analysed, so these would be natural extensions. Further, extending the BVI to also handle multiplicative overdispersion would allow the user to specify if the overdispersion should be modelled as additive or multiplicative and would be a valuable addition. 
\\
\\
It was also desirable in \citet{Arnstad}, to go deeper in to the theoretical properties that the BVI method possesses. As the BVI method is first and foremost a tool for researchers, the main focus of this thesis was put on developing a credible variable importance measure and wrap this in an R package so that it could be applied. Due to the complexity of this, the time and resources did not allow for a full theoretical breakdown of the BVI method. Such analysis would be of very high interest, in particular some proofs in expectation for the variable importance estimates would be helpful, to further solidify the credibility of the method. 
\\
\\
We did not consider random slopes when developing the BVI method, but this could also be a possibility for further work. As the random slopes are often associated with a fixed effect, the correlation structure one obtains with random slopes is much more complex than that of random intercepts. This could be a difficult challenge to implement, but as discussed in \Cref{sec:R2_LMM}, the proposal by \citet{Johnson2014} could be a good starting point. One should however be careful to make sure that the random slope improves the model significantly, and that this improvement outweighs the potential computational burden including random slopes could have.
\\
\\
Conceptually, variable importance is in itself a debated topic. The first question one can ask is what the definition of relative importance is. In \citet{gromping_relaimpo}, relative importance is based on variance decomposition and we have chosen to follow this notion. However, this definition has the disadvantage that an agreement of allocation of importances for correlated covariates seems impossible \citep{Gromping_2015}. This problematic issue is present in our results when the fixed effects were correlated, making evaluation of the method difficult. For our method, the pattern observed was a consequence of the relative weights method, rather than a general method for distributing the shared variance between covariates. The search for a unified variable importance framework has given us methods such as the LMG \citep{gromping_relaimpo}, Proportional marginal variance decomposition (PMVD) \citep{gromping_relaimpo}, the relative weights method \citep{johnson_relative_weights} and dominance analysis methods \citep{budescu1993dominance}. Yet, no one has been able to provide a method that is completely accepted by the field of mathematics. For these reasons, variable importance as a subject, and its methods, have received criticism \citep{gromping_relaimpo}. However, we believe that variable importance methods can give researchers valuable information and spark ideas, and that they therefore should have a place in the statistical toolbox. That being the case, we wish to emphasize that all statistical methods are limited by the assumptions they rely on and the data they are applied to. As \citet{Sutherland_91} put it; \textit{Statistical techniques do not build theory - theoreticians do}.


\begin{itemize}
    \item Summarize the method. Similar to that of the project thesis. 
    \item State that it can be used across diciplines, and that a Bayesian approach is useful when prior information is available.
    \item Now the discussion really begins. State that the methodology for LMMs proved to imply that we have a proper decomposition of the $R^2$. Even though this was not a main focus in this thesis, the results of uncorrelated covariates and marginal and conditional $R^2$ values seem to be in line with the what one would expect. 
    \item State that we have addressed two main points from the project thesis, namely testing the methodology on real data and expanding it to handle GLMMs.
    \item Further, we allow for a covariance structure in the random effects, which is not possible in the project thesis.
    \item Emphasize that the results in this thesis are calculated based on theory from a subject that in itself has been subject to criticism. Therefore, the results should be interpreted with caution, especially when we also use the definitions of $R^2$ from Nakagawa, which may be oversimplified. (See last discussion section of project thesis)
\end{itemize}

Further work:
\begin{itemize}
    \item Extend the package to encompass the models with known distributional variance as in \citet{nakagawa2013general} and \citet{nakagawa2017}.
    \item No proofs were considered due to time limitation
    \item Random slopes could be featured, at least if one can say they improve the model.
    \item Correlated random effects (?)
\end{itemize}



HUSK Å NEVNE AT DET ER HELT NATURLIG AT VI FÅR RESULTATER SOM VARIERER LITT FRA GANG TIL GANG. DETTE KAN FORKLARE HVORFOR VI FOR EKSEMPEL IKKE TREFFER STOFFELS ESTIMATER OG BIOLOGENES ESTIMATER, MEN, BASERT PÅ SIMULERINGSSTUDIEN, FØLER VI OSS TRYGGE PÅ AT DENNE SPREDNINGEN IKKE ER FOR STOR, OG AT EN KJØRING AV METODEN KAN SEES PÅ SOM EN TILFELDIG PRØVE FRA EN FORDELING SOM ER SENTRERT RUNDT DEN KORREKTE VERDIEN.

LEGG TIL TO GITHUB LINKER, EN FOR PAKKEN OG EN FOR MASTEREN.

KAN DISKUTERES OM VI BURDE UNDERSØKT BEDRE PRIORS FOR ANIMAL MODEL OG DE ANDRE SIMULERINGENE
KAN DISKUTERES OM VI BURDE BRUKT HØYERE KORRELASJON, MEN DA TROR JEG IKKE MODELLENE VILLE BLITT KONVERGENTE
JEG TROR METODEN HELT FINT KLARER KATEGORISKE KOVARIATER NÅR DUMMY ENCODING BRUKES

DISKUTER SHRINKAGE PÅ RANDOM EFFECTS MED POSITIVT KORRELERTE FIXED EFFECTS OG INCREASE PÅ NEGATIVT KORRELERTE
DISKUTER HVORFOR POISSON MED HØY KORRELASJON GIR SÅ RARE RESULTATER

All code used to produced the presented results can be found on the authors Github, and a link to the repository is provided in \Cref{ap:github-repository_thesis}. In the Github, the fully developed package is available, with all files found in the repository linked in \Cref{ap:github-repository_package}. To make it easy to apply, the author has provided a usage example of the package in \Cref{ap:bayesian-importance}. This covers installation, simulates data, formulates and fits a model before drawing samples and obtaining relative importance plots and summary statistics. 

