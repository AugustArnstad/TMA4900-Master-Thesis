Based on the presented background theory, we now present our novel method for combining this into a relative variable importance tool for Bayesian GLMMs called PLACE NAME HERE. The method applies to GLMMs modelled with Binomial, Poisson and Gaussian responses, and assumes the distinct random effects to be independent. Random slopes are not included in the method. 
% \subsection{Generalizing the relative weights method to GLMMs} 
% The presented theory on relative variable importance has mostly been developed for linear regression models. However, given the assumption of independence between fixed effects and random effects, we see no reason why the theory cannot be extended further. In \citet{matre}, the relative weights method was extended for the single random intercept model and showed promising results. Further, as long as the different random effects are independent, we argue that the variance estimates, when scaled by the response variance, directly correspond to their variance contribution. Utilizing the relative weights method for the fixed effects in a random intercept model using INLA was done in \citet{Arnstad} and a simulation study indicated that this method provides a proper decomposition of the $R^2$.
% DO I NEED TO BACK THIS LAST STATEMENT UP? IT IS BASICALLY "MY OPINION".
\\
\\
If categorical covariates with more than two levels are contained in the fixed effects, they should be encoded using distinct names in order to make sure the code runs. Place this in the documentation of the code!!!
\section{Variable importance in the Bayesian framework}
There are a few considerations necessary in order to calculate variable importance on GLMMs in a Bayesian framework. First of all, the characteristics of the Bayesian framework must be considered. When fitting a GLMM in the frequentist framework, point estimates of the coefficients of the fixed effects as well as point estimates of the variance from the random effects are obtained. These estimates are then used to calculate relative variable importance measures. In contrast, a Bayesian GLMM tries to estimate the joint posterior distribution of parameters. From the posterior distribution, one can obtain samples of all parameters, that can be used to approximate a posterior distribution for each parameter. It is these samples that will be used for further calculations.
% \\
% \\
% Further, the presented theory on relative variable importance has been developed for linear regression models. 
% However, given the assumption of independence between fixed effects and random effects, we see no reason why the theory cannot be extended further. In \citet{matre}, the relative weights method was extended for the random intercept model and showed promising results. The Bayesian analogue in \citet{Arnstad} utilized the relative weights method for the fixed effects in a random intercept model and a simulation study indicated that this method provides a proper decomposition of the $R^2$. 
% Moreover, as long as the distinct random effects are independent, we argue that the variance estimates, when scaled by the response variance, directly correspond to their proportion of explained variance. From this, we believe that the relative weights method can be applied to the fixed effects also in a GLMM.
\\
\\
Secondly, we argue that the most intuitive way to calculate variable importance is on the link (or latent) scale. The reasoning behind this is the definition of residual variance with additive overdispersion in \citet{nakagawa2013general}. This definition makes GLMMs analogous to LMMs, thus supporting a unified approach to both types of models. Therefore, we consider only GLMMs modeled with additive overdispersion, although we believe the method could be extended to handle multiplicative overdispersion as well.
These considerations are the basis for our proposed model for calculating relative variable importance in Bayesian GLMMs.
The presented method does not include categorical variables with more than two categories or random slopes. The exclusion of categorical variables are seen as outside the scope of this thesis, whereas the random slopes are omitted due to the added computational complexity and the debatable improvement of GLMMs and $R^2$ values with random slopes as mentioned in \citet{Johnson2014}.
We now go in to detail on how the different components of the model are handled in our method.

% \subsection{Generalizing the relative weights method to } 
% Utilizing the relative weights method for the fixed effects in a random intercept model using INLA was done in \citet{Arnstad} and a simulation study indicated that this method provides a proper decomposition of the $R^2$.
% Given the assumption of independence between fixed effects and random effects, this method can be extended to any regression model, assuming appropriate considerations for a link function and random effects are addressed.
% DO I NEED TO BACK THIS LAST STATEMENT UP? IT IS BASICALLY "MY OPINION".

\section{Extending the $R^2$ to Bayesian GLMMs\protect\footnote{A method for calculating the $R^2$ for Bayesian LMMs was proposed in \citet[Chapter 2]{Arnstad}, however we see it fitting to include this extension in the methods chapter as it has been developed by the author for this thesis.}}
\label{sec:R2_Bayes_GLMM}
The basis of our variable importance measures is a decomposition of the $R^2$ value so that each covariate is assigned a share of relative variable importance. We now combine the definition of the $R^2$ for GLMMs presented \Cref{sec:R2_GLMM} and the $R^2$ for the Bayesian linear regression from \Cref{sec:bayes_R2} to yield our proposed distribution of the $R^2$ for Bayesian GLMMs. Consider the linear predictor 
\begin{equation}
    g(\boldsymbol{\mu}) = \boldsymbol{\eta} = \mathbf{X}\boldsymbol{\beta} + \mathbf{U}\boldsymbol{\alpha} \ ,
\end{equation} 
for some monotonic and differentiable link function $g(\cdot)$. 
The variance components of the linear predictor can be decomposed into variance from the fixed effects and the random effects. Define the variance of the fixed effects as 
\begin{equation}
    \sigma_{f}^2 = \text{Var}(\mathbf{X}\boldsymbol{\beta}) \ ,
\end{equation}
and let $\sigma^2_{\alpha_i}$ denote the variance of the $i$-th random effect. For Gaussian responses corresponding to an LMM, the residual variance is defined as $\sigma^2_{\varepsilon}$ and is explicitly modelled. However, for non-Gaussian responses, the residual variance of the model when considering additive overdispersion is defined as
\begin{equation}
    \sigma_{\varepsilon}^2 = \sigma^2_e + \sigma^2_d \ ,
\end{equation}
where $\sigma^2_e$ is the additive dispersion and $\sigma^2_d$ is the distributional variance. A table containing the distributional variances for the link functions used in this thesis can be found in \Cref{table:1}. Given that we can obtain samples for the variance components, we define for a sample $s$ the marginal and conditional $R^2$ for the Bayesian GLMM as
\begin{equation}
    \label{eq:R2_Bayes_GLMM}
    R^2_{s, m} = \frac{\sigma_{f, s}^2}{\sigma_{f, s}^2 + \sum_{i=1}^q \sigma_{\alpha_i, s}^2 + \sigma_{\varepsilon, s}^2} \quad \text{and} \quad R^2_{s, c} = \frac{\sigma_{f, s}^2 + \sum_{i=1}^q \sigma_{\alpha_i, s}^2}{\sigma_{f, s}^2 + \sum_{i=1}^q \sigma_{\alpha_i, s}^2 + \sigma_{\varepsilon, s}^2} \ ,
\end{equation}
respectively, where $\sigma_{\varepsilon, s}^2 = \sigma^2_{e, s} + \sigma^2_d$ is the sampled residual variance and $\sigma^2_d$ is distribution specific and the same for all samples. The posterior distribution of the $R^2$ will then be approximated by the distribution of the samples of $R^2_{s, m}$ and $R^2_{s, c}$ for $s=1, ..., S$.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Distribution} &  \textbf{Link Function} & \(\sigma^2_d\) \\
    \hline
    Binomial & Logit & \(\pi^2/3\) \\
    \hline
    Binomial & Probit &  1 \\
    \hline
    Poisson & Log &  \(\ln\left(1 + 1/\exp\left(\beta_0 + 0.5 (\sum_{k=1}^q \sigma_{\alpha_k}^2 + \sigma^2_e)\right) \right) \) \\
    \hline
    Poisson & Square Root &  0.25 \\
    \hline
    \end{tabular}
    \caption{Distribution-specific variance \(\sigma^2_d\) for the Binomial and Poisson distributions for the link functions implemented in this thesis. The variances correspond to the values in \citet{nakagawa2013general} and the calculation for the log-link Poisson follow the recommendations of \citet{nakagawa2017}.}
    \label{table:1}
\end{table}

    
    


\section{Decomposing the $R^2$ value}
We now seek to decomposed the proposed $R^2$ value and assign each covariate with a proportion of the variance explained, i.e. assign each covariate with a \textit{relative variable importance}. Recall that the fixed and random effects are assumed to be independent, so that one can consider the variances of the fixed and random effects separately. Further, the residual variance is also considered as independent of both fixed and random effects. 
\subsection{Applying the relative weights method in the Bayesian framework}
To remedy the problems of calculating importance of correlated covariates, we will apply the relative weights method to the fixed effects before fitting the model. Following \Cref{sec:relativeweights}, we project the design matrix $\mathbf{X}$ of the fixed effects to obtain the matrix $\mathbf{Z}$. The model is fit using $\mathbf{Z}$ as an approximated design matrix of fixed effects, and from the joint posterior distribution samples of the coefficients $\boldsymbol{\beta}_{\mathbf{Z}}$ can be drawn. Each sample $\boldsymbol{\beta}_{\mathbf{Z}, s}, s=1, ..., S$ can be used to approximate a sample of the importance of the columns $\mathbf{X}$. Using equations \eqref{eq:lambda} and \eqref{eq:RI_lambda}, we calculate this sample as
\begin{equation}
    \text{IMP}(\mathbf{X})_s = \boldsymbol{\Lambda}^{[2]} \boldsymbol{\beta}_{\mathbf{Z}, s}^{[2]} \ ,
\end{equation}
where $\text{IMP}(\mathbf{X})_s$ is a column vector containing the approximated importance of column $k$ of $\mathbf{X}$ on the $k$-th entry for $k=1, ..., p$. To calculate the relative variable importance, note that we estimate $\sigma^2_{f, s}$ in \eqref{eq:R2_Bayes_GLMM} by
\begin{equation}
    \sigma^2_{f, s} = \sum_{k=1}^{p}\text{IMP}(\mathbf{X})_{s, k}  \ . 
\end{equation}
Therefore, we define the relative importance of column $k$ of $\mathbf{X}$ as
\begin{equation}
    \text{RI}(\mathbf{X})_{s, k} = \frac{\text{IMP}(\mathbf{X})_{s, k}}{\sigma_{f, s}^2 + \sum_{i=1}^q \sigma_{\alpha_i, s}^2 + \sigma_{\varepsilon, s}^2} \ ,
\end{equation}
where $\sigma_{\alpha_i, s}^2$ and  $\sigma_{\varepsilon, s}^2$ are defined as in \Cref{sec:R2_Bayes_GLMM}.
For sufficiently large $S$, these samples can be used to construct an approxmation of the posterior distribution of the relative importance for each fixed effect. 

\subsection{Random effects}
The presented theory on relative variable importance has mostly been developed for linear regression models. As long as the random effects are assumed not to be correlated, introducing random effects does not change the general idea. For each random effect, an approximation of the posterior distribution is constructed from the samples of the joint posterior distribution. Then, the proportion of variance explained by random effect $i$ is calculated as 
\begin{equation}
    \text{RI}(\alpha_i)_{s} = \frac{\sigma_{\alpha_i, s}^2}{\sigma_{f, s}^2 + \sum_{k=1}^q \sigma_{\alpha_k, s}^2 + \sigma_{\varepsilon, s}^2} \ .
\end{equation}
In addition to the relative importance of the random effects, a quantity of interest is the intraclass correlation, often also called the within cluster correlation or repeatability \citep{GLMM_book}. The ICC represents the correlation between observations within the same cluster, and is defined for a random effect $\boldsymbol{\alpha}_i$ in \citep{nakagawa2017} as
\begin{equation}
    ICC = \frac{\sigma_{\alpha_i}^2}{\sum_{k=1}^{q}\sigma_{\alpha_k}^2 + \sigma_{\varepsilon}^2} \ .
\end{equation}
Thus, following the same logic as before we can sample the ICC as 
\begin{equation}
    \text{ICC}_s = \frac{\sigma_{\alpha_i, s}^2}{\sum_{k=1}^{q}\sigma_{\alpha_k, s}^2 + \sigma_{\varepsilon, s}^2} \ ,
\end{equation}
and obtain an approximate posterior distribution of the ICC.
%In quantitative genetics, the ICC is of particular interest as it corresponds to the heritability of a trait.

\section{Heritability of phenotypic traits}
A particularly interesting application of variable importance, and an area of much active research, is estimating the heritability of phenotypic traits. As mentioned in \Cref{sec:animalmodel}, heritability is defined as the ratio of additive genetic variance to total phenotypic variance \citep{Wilson_heritability}. When modeling a phenotypic trait as the response, the variable importance of the random effect accounting for additive genetic variance can be interpreted as the heritability of the phenotypic trait. Therefore, this is a useful application of our variable importance method, and has been the motivation for the development of the method.
% The parameters of interest are now the additive genetic variance $\sigma^2_A$ and the \textbf{heritability}, defined as the proportion of the total phenotypic variance that is present due to the additive genetic variance, $\sigma^2_A/\sigma^2_P$ \citep{Wilson2008}.

% \section{Case studies}
% To illustrate and evaluate our proposed method, we perform several case studies. The first case study investigates how the method performs on real data from a study on house sparrows (\textit{Passer domesticus}) and compares to the study in \citet{Stensland_GMRF_bayes_animal_model}. The second case study applies the method to a Gaussian, a Poisson and a Binomial GLMM, and compares the results to the vignette of the \texttt{rptR} package found at \url{https://cran.r-project.org/web/packages/rptR/vignettes/rptR.html} and described in \citet{Stoffel2017rptR}. The two aforementioned case studies investigate the heritability of traits and repeatability of clusters respectively. 
% \\
% \\
% Either:
% \\
% \\
% Therefore, we perform a third case study on a simulated dataset, in which we know the true values and can therefore evaluate the variable importance for all parameters. In this study, we model the Poisson and Binomial GLMMs, and refer to \citet{Arnstad} for a simulation study on Gaussian LMMs using the same methodology.
% \\
% \\
% Or:
% \\
% \\
% For calculations of the importance on other covariates, we refer to the simulation study in \citet{Arnstad}. This study covers the method for the LMMs and has shown promising results of a proper decomposition of the $R^2$ value.

\subsection{House sparrow study}
We now apply our method to a dataset gathered on house sparrows (\textit{Passer domesticus}) from a study on the coast of Helgeland, Norway \citep{Stensland_GMRF_bayes_animal_model}. The entire bird population on five islands have been surveyed since 1993 and several morphological traits have been measured. Blood samples were drawn to determine the relatedness between birds and we therefore have a pedigree structure for the birds \citep[citing Jensen et al., 2003, 2004, 2008]{Stensland_GMRF_bayes_animal_model}. In the dataset we use we have $N=3116$ birds with one or more observations on the traits and covariates. For a more thorough description of the house sparrow study, see \citet[and references therein]{Stensland_GMRF_bayes_animal_model}. We model three traits using a Gaussian LMM, namely the body mass, wing length and tarsus length. The fixed effects in the model consist of observations of \textit{sex}, a standardized inbreeding coefficient denoted \textit{FGRM}, the standardized \textit{month} of the year (measurements were made during May-August), the \textit{age} of each bird, and dummy variables encoding the location of the \textit{native island} group of the bird (three levels, outer islands, inner islands or other islands). In addition, we model the \textit{hatchyear}  as an independent and identically distributed (i.i.d.) random intercept. To account for the correlation between relatives, we include a random effect for the pedigree structure, specifying the relatedness matrix as the correlation structure. Lastly, to account for individual differences we add an i.i.d. random intercept for the individual bird. We prefer to use the INLA framework, described in \Cref{sec:INLA_framework}, to fit our LMM as it is computationally efficient and easy to use.

\section{Non-Gaussian case studies}
\subsection{Binomial and Poisson case studies}
To investigate how well our method generalizes to non-Gaussian responses, we perform a case study using the setup described in the vignette of the R-package \texttt{rptR} \citep{Stoffel2017rptR}. The dataset, introduced for a different purpose, is simulated to replicate a study on twelve different beetle larvae populations \citep{Stoffel2017rptR}. It contains the covariates \textit{population}, the discrete \textit{habitat} of the larvae, the dietary \textit{treatment} of the larvae, the \textit{sex} and \textit{container} of which the larvae was contained in. The phenotypes to be modeled by the Binomial and Poisson distributions are the two distinct male colour morph and the number of eggs laid by female larvae respectively. Both models use treatment as the only fixed effect and place i.i.d. random intercepts on the population and container covariates. As before, our modelling is carried out using INLA, whereas the models in the vignette are calculated from functions in the \texttt{rptR} package. 


\subsection{Binomial and Poisson simulation studies}
As the case studies previously described all consider the heritability or repeatability as the quantity of interest, we simulate data to be able to evaluate the performance of our method when calculating relative variable importance for all parameters. We simulate $N=10000$ responses from a Poisson distribution with log-link, a Binomial distribution with a logit link and a Binomial distribution with a probit link. The linear predictor contains three fixed effects and one random intercept. The fixed effects $\mathbf{X} \sim \mathcal{N}(\boldsymbol{\mu}, \Sigma)$ with $\boldsymbol{\mu} = (0, 0, 0)^T$, $\Sigma_{i, i} = 1$ and $\Sigma_{i, k} = \rho, k \neq i$. The true regression coefficient are set to be $\boldsymbol{\beta}=(1, \sqrt{2}, \sqrt{3})^T$. The random effect comes from $m=100$ clusters, each with $n_j=100$ observations for $j=1, ..., m$. Further, we draw the random effect from a normal distribution with mean zero and variance $\sigma^2_{\alpha_1} = 1$. All data is standardized before fitting the models and we fit four different models letting $\rho$ vary for each model by taking on the values $0, 0.1, 0.5$ and $0.9$. The INLA framework is used to fit the GLMMs and the methodology described used to calculate the relative importance.
\\
\\
For the binomial simulation with distributional variance $\sigma^2_d$ independent of the fitted model, we can empirically calculate the relative importance of the parameters when they are not correlated. When uncorrelated, the proportion of variance explained by each covariate in the linear predictor is equal to the square of the true coefficient. By defining $\sigma_{x_k}^2$ as the variance contribution to the linear predictor for covariate $k$, we then have 
\begin{equation}
    \sigma_{x_1}^2 = \sigma_{\alpha_1}^2  = 1 \quad \text{and} \quad \sigma_{x_2}^2 = 2 \quad \text{and} \quad \sigma_{x_3}^2 = 3 \ .
\end{equation}
Then, the relative importance of the covariates can be calculated as
\begin{equation}
    \begin{aligned}
        \text{RI}(\mathbf{X})_{1}  = \text{RI}(\alpha_1)  &= \frac{\sigma_{x_1}^2}{\sum_{i=1}^{3}\sigma_{x_i}^2 +\sigma_{\alpha_1}^2  + \sigma_d^2}, \\
        \text{RI}(\mathbf{X}_2) &= \frac{\sigma_{x_3}^2}{\sum_{i=1}^{3}\sigma_{x_i}^2 +\sigma_{\alpha_1}^2 +  \sigma_d^2}, \\
        \text{RI}(\mathbf{X}_3) &= \frac{\sigma_{x_3}^2}{\sum_{i=1}^{3}\sigma_{x_i}^2 +\sigma_{\alpha_1}^2 +  \sigma_d^2} \ .
    \end{aligned}
\end{equation}
This means that the expected relative importance in the binomial model with probit link of $X_1$ and $\alpha_1$ is $1/8$ and the expected relative importance of $X_2$ and $X_3$ is $2/8$ and $3/8$ respectively. For the logit link we have an expected relative importance of $0.0972$ for $X_1$ and $\alpha_1$, $0.194$ for $X_2$ and $0.292$ for $X_3$.
For the Poisson simulation however, the distributional variance is approximated by $\sigma_d^2 = \ln (1 + 1/\mathbb{E}[\lambda])$ with $\mathbb{E}[\lambda]=\exp\left(\beta_0 + 0.5 (\sum_{k=1}^q \sigma_{\alpha_k}^2 + \sigma^2_e)\right)$, and is therefore dependent on the fitted model \citep{nakagawa2017}. 

% All data is standardized before fitting the models, so that 
% \begin{equation}
%     \sigma_{x_1}^2 = \sigma_{\alpha_1}^2 = \sigma_{\alpha_2}^2 = \frac{1}{9}
% \end{equation}

% The total variance of $\boldsymbol{\eta}$ from this setup can be found as 
% \begin{equation}
%     \text{Var}(\boldsymbol{\eta}) = \beta_{1, \mathbf{X}}^2 + \beta_{2, \mathbf{X}}^2 + \beta_{3, \mathbf{X}}^2 + 2\sum_{j=1}^{3}\sum_{k=j+1}^{3} \beta_{j, \mathbf{X}}\beta_{k, \mathbf{X}}\rho_{jk} + \sigma_{\alpha_1}^2 + \sigma_{\alpha_2}^2 + \sigma^2_{\varepsilon} \ .
% \end{equation}
% For the uncorrelated case ($\rho=0$) we can find that this equals 
% \begin{equation}
%     \text{Var}(\boldsymbol{\eta}) = 1 + 2 + 3 + 1 + 1 + 1 =  9 \ ,
% \end{equation}
% meaning that in the linear predictor we have 
% \begin{equation}
%     \text{RI}(\mathbf{x}_1) = \text{RI}(\alpha_1) = \text{RI}(\alpha_2) = \frac{1}{9}, \quad \text{RI}(\mathbf{x}_2) = \frac{2}{9}, \quad \text{RI}(\mathbf{x}_3) = \frac{3}{9} \ .
% \end{equation}
% We must now account for the distributional variance, which is independent of the relative contributions. 

% and we must now define the distribution specific variance. For the Poisson distribution with log-link, we approximate the distribution specific variance as in \Cref{table:1} with values obtained from the fitted model. The binomial distribution with probit link is assigned a distributional variance of $1$.
% \begin{equation}
%     \sigma^2_d = 
%     \begin{cases} 
%         \ln\left(1 + \frac{1}{\exp(2)}\right) = 0.127 & \text{for the log-link}, \\
%         1 & \text{for the probit link}.
%     \end{cases}
% \end{equation}
% and therefore we can calculate the relative variance contribution on latent scale, i.e. how we calculate relative variable importance, for each covariate as 
% \begin{equation}
%     \text{RI}(\mathbf{x}_1) = \text{RI}(\alpha_1) = \text{RI}(\alpha_2) = \frac{1}{\text{Var}(\boldsymbol{\eta})}, \quad \text{RI}(\mathbf{x}_2) = \frac{2}{\text{Var}(\boldsymbol{\eta})}, \quad \text{RI}(\mathbf{x}_3) = \frac{3}{\text{Var}(\boldsymbol{\eta})} \ .
% \end{equation}
% \\
% \\
% We fit four different models, letting $\rho$ vary for each model by taking on the values $0, 0.1, 0.5$ and $0.9$. The INLA framework is used to fit the GLMMs and the methodology described used to calculate the relative importance.

% \begin{table}[ht]
%     \centering
%     \begin{tabular}{lrr}
%     \hline
%     $\rho$ & $R^2_{\text{marg}}$ & $R^2_{\text{cond}}$\\ 
%     \hline
%     $0$ & $0.750$ &  $0.875$ \\ 
%     $0.1$ & $0.781$ & $0.890$ \\ 
%     $0.5$ & $0.852$ & $0.926$\\ 
%     $0.9$ & $0.889$ & $0.945$\\ 
%     \hline
%     \end{tabular}
%     \caption{The theoretically correct marginal variance explained (left column) and conditional variance explained (right column) for different correlation levels between the fixed effects.}
%     \label{table:2}
% \end{table}

